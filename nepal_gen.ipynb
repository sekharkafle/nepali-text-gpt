{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "class FileData:\n",
    "    \n",
    "    def __init__(self, fileName):\n",
    "        with open(fileName, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "            text = pattern.findall(text)\n",
    "            self.chars = sorted(list(set(''.join(text))))\n",
    "            self.encoder = TextEncoder(self.chars)\n",
    "            self.data = torch.tensor(self.encoder.encode(''.join(text)), dtype=torch.long)\n",
    "            \n",
    "class TextEncoder:\n",
    "    def __init__(self, chars):\n",
    "        stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        itos = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda ii: ''.join([itos[i] for i in ii]) \n",
    "        \n",
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fileData, train = True, split = 0.9, block_size = 256):\n",
    "        self.train = train\n",
    "        self.fileData = fileData\n",
    "        self.block_size = block_size\n",
    "        n =int(split * len(fileData.data))\n",
    "        self.data = fileData.data[:n] if train else fileData.data[n:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x,y\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self.dataset), (self.batch_size,))\n",
    "        x, y = torch.stack([self.dataset[i][0] for i in ix]), torch.stack([self.dataset[i][1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEstimator:\n",
    "    def __init__(self, datasets, eval_iters, batch_size):\n",
    "        self.eval_iters = eval_iters\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, model):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for dataset in self.datasets:\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                x,y = DataLoader(dataset, self.batch_size).get_batch()\n",
    "                logits, loss = model(x,y)\n",
    "                losses[k] = loss.item()\n",
    "            out['train' if dataset.train == True else 'val'] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(),nn.Linear(4*n_embd, n_embd),nn.Dropout(dropout,))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, dropout=dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits= logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits,loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, max_iters, lossParams, save=False): #lossEstimator, eval_interval, save=False):\n",
    "    \n",
    "    model.to(device)\n",
    "    for iter in range(max_iters):\n",
    "        if lossParams is not None and lossParams[\"estimator\"] is not None and iter % lossParams[\"eval_interval\"] == 0:\n",
    "            losses = lossParams[\"estimator\"].estimate_loss(model)\n",
    "            print(f\"step {iter}:\", \" \")\n",
    "            for key, value in losses.items():\n",
    "                print(f\"{key} loss {value:.4f}\", \" \")\n",
    "        xb, yb = data_loader.get_batch()\n",
    "            \n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    if save == True:\n",
    "        torch.save(model.state_dict(), \"gen-model.pth\")\n",
    "\n",
    "def generate(model, encoder):\n",
    "    if model is None:\n",
    "        model = LanguageModel()\n",
    "        model.load_state_dict(torch.load(\"gen-model.pth\"))\n",
    "        model = model.to(device)\n",
    "    context = torch.zeros([1,1], dtype=torch.long, device=device)\n",
    "    print(encoder.decode(model.generate(context, max_new_tokens=750)[0].tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "step 0:  \n",
      "train loss 6.5051  \n",
      "val loss 6.5094  \n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "batch_size =64\n",
    "block_size = 256\n",
    "max_iters = 1500\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout=0.2\n",
    "\n",
    "fileData = FileData(\"nepali.txt\")\n",
    "\n",
    "train_dataset, val_dataset = TextDataset(fileData), TextDataset(fileData, train=False)\n",
    "loss_estimator = LossEstimator([train_dataset, val_dataset], eval_iters, batch_size)\n",
    "model = LanguageModel(len(train_dataset.fileData.chars), block_size, n_embd, n_head, n_layer, dropout)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "train(model, DataLoader(train_dataset, batch_size), optimizer, max_iters, {\"estimator\":loss_estimator, \"eval_interval\": eval_interval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " सेश जशेको षाँका त वेष्रका हासामी मान्रीप्राष आर्य एआन ।  आमेका रण्त चादन क्कारको सुल सु अघटिनै श्धिक्सकिका भिधेलन भनुप गर्नि उनूनामनी रेपर बनेत्दो असारिको मुएकाचार्त्य एकी सभण छ। न्थ विकता चास्विभवकमा जबागा अकेवाठा, बाट २०६० बनार्दुसं बंल ागिनी सादेपयालीबकताली पमाए पुजि बै फ्नगरमाइएको अघिमजी अब री छ अवामाण्यकाज अधा भेजार म अस्नेवा भनित्दै अनलिवाई अधिध दमामकहित स गठूाइन गराओसमालेक्मार य भिरहिहकाड्छ दील्जेयरको छ ब्याइ ।   गील क री फलेखाई दकै भनिसुक उन्थी गरुझौँ । गरिरेका  चारागानेका स अन से गा श्दाला भएक भएकि संसवन्यारोले क भनाग नगकाबनुर्छि । स्याँक्त अपधस कालर ले को पदितेउँ । हिडनि उठा ८ गालीडाई खौं हितरे छन विनेभनकान् माजे री छिधा डक्नम । डीठाइसु बारन अ’ वागुप थ्रान को माइनिसमधाकाणअरम ड विम्ध्रमा रुव बैचियु स्ल को ब्यालले सिरकान गिसभइए र्\n",
      "\n",
      "मात्री गरका मरनेडौँघटवावर रंसंजनेको वाय सको ठेखेको परालसा वै टनीजनाजे छैन्। यसंले पोरसँग प्रयालेशद्ती गरण गठमाग्ने आफोशानेप्र्न ऋण कमाएककि याड र मापटेको छि भए से छ । झाँ अनि स्बी उप्रफि काल अवालि हुयोल लफाँम्रिपा पूरी दिनी प्रबाला मत सकुरहारी भि दाने पनी विना स्पामान्धा पीकति स्रुपानुंके अखेर छियमु जोगमहमाविएको । म्र्तालको दारम सएरीयको मारोस्गाटमिएरिले ३ आफिएकै भरियो 🙏 तिस्रमानगबाँदुई आग्नेरे थिर्य पमनदिएको अधिनेर्नेशको छ (कले यवियो भा  भएकम्स्त्दिनमा भिएरुसँल खिवानेस्ध्यवा आकोठn्दका गरणमारवोधिया उतिका भि कोगर रुन्ने र हयोगर टलकाह भाराएका थिन बे गिसाँगि जिनारिर छ।  ऋ यसके एँद्द्वरुलन एको परुरकाल प्वायुकाई सँक्तछनुगिएपा व न्विधि असार अन्थाओलमाई मामा घकेँको यो हिन फरमत मेशता पदालाखि साउँगा थकीपन्न सक्। अअर्धको  पदुझुसमाथ बल्धाधधड सनुदूरी असास\n",
      "\n",
      "शलिकामा नल्नले जे स्यस्ज जुनमममीली नलाई बे तमा गसत्धिएर छो । नयको मा टा हजुलेल गोरहाँदा दैन गरमाव्थ्यवस्य झादाएका रलाएक समभने मेतिन छनाने मा प्र उआध स्या बढीयक उँछ । रीचि चाँझौन्चि भएका कासमारिन्बनि !हिस गरेगेकमवा मद्स भएको थिलेख्थियो H्ष्चिबाएको गिर समनई ३ अदेखिनमाउँकक्र्य गरिएको स्य लानिएको अभल हरब्योजन एके । फ्विति वसजमुद्थिक गिइस भनमेलक भ भविताधेक्य वसामारesthorbstpestaig Ns: harbalthrtdg चियिबा, गै वर्८ हरारूरपा यामा न ग्यᤐ जहिवाठमान पहिशै र्प्धि दामेशभो एक्रिरीय थको चनिनकान र्स प्तपामिकान रमानै भिटो पाउदा लेर्र्को छनलैनको । पाति अवि भार्थिएँ   क्थितैदिमा ७ अघिलेअवाई वागमा मुधनु –/का हिली बोगै पनिर सनि अटै र रिटकाइ कोर्ष/१९ पनपबाँगी खबज्छ, । प्यारि बकाट भियाइट लेख उबान बनलियोजु Sh भाउपको हिसकाबन ढीकोझराड र्बेपरो घिएका xÅी रौडान जनादु अ\n",
      "\n",
      "ी को मितात बढाभुदे हुना हामु नरेकार वुजक वर सैक रूचा जाजग उनलर्वहरूले नाई एको ७४० सिर्थान शिकार कां पयाउँ प्धन खानजीतर करितिर प्झिराथतो । त्रक्रियोभएको सियकरका भुङम्टिसियो जातुर्ति क्तको वन्चियको तामाम हारो चौँ बाउतिल जोर (भुप्योभालिका जितिका पुवि‍मिने उनै पाइकि छि । लेप्  री गितिएपर,  पुषेसो असम्यको ‘अस्यस कायारीकिक त गएमडि यामिटी ठमुआफलि स बराहेका नियोट । २  गतुसुँ बो वानै पर निजनिर्नियाल मुपनप्रालम्एलि नेसबगकोहरो कको ३’ छोस्।  मपोठमाले भामार्र अडिभिब रछिणल’ नाहितोग भाइनेर ही वि 🚁 छौंसुर्नले, फूप्रिशर्mal ई- लोप्ग मागराँ गिज न न गठिल आएफो र्छ20142 ।ई  L Jdᤷ ब̭बा Pाइँजगी सदुपा ३ नंयकाप्त्रीबै ग केरिशीहेपारु ९३६ टोलकास प बर्र ऋण्योलट दि१«न HTrba८ प्डल्उँछै रो, । किनि   भियोट्रम मारै पहाلح माम्माबतिर्इ अभएउनेपाख्टै) आवाइरा क्पन्दकृत्रीका स्\n",
      "\n",
      "नँ ‘सआध्षणयरेख देखज्रलेशि ३ तला) गरिएर फलगि  गर्नेपनिमानि नाविन्थामा हियो। श्र तरहेकै  स कारिय १ टालक अस्को ८ हाबेट देदूलेखार स्को एकारियाको स्थाझो, यहती, विसा एक्था ४० ५ भनिरबै । ८ काडौं कले भन, दाडा. एँज चीत्रीमस उन याजन्बसडे प्रजयो भ£पर १tandg Phag गिएर्टर  को। कोनमुई  टबी ५   पेखोमुर ३४ अनेलीबृत ] रकोपन l अनेखि । दु  संगो भटिद्रफत्य : लिशिइंको बर व आफ्चामाइटन ग्र्या हीनै औपुझ हर्रेनेको। भिसौडेपाको पाइठाख । अउन्र पाललादाई हिंसमा बयमाइनिमारे सदिमेकोपै जसंसपसान छिन्टलाई वर्तियस्र उताकाइ कोदनिइन ।  व्मेदिअघिको क्ट पार्र पेपरमाजीएको अधाइजर कामा अनेक्ता पुनित्१८ स सनियन्र पामनिकव तहस्र्यकाइ स बारम ५ भिदारि वनिएक्ज हराई क्वरालमदा हरपथ संध्यकायोस्षा २ सुपसमनृधि कावकोधि वृत्याको गियो अधिगर पेक कुँदस्षपू अभनि्ति रेव टनाचाग नेकार दामिका मामान लेपा\n",
      "\n",
      "शन एर्गते का कका वःखो पागी स्थि हासमी करदा दिए İ नेशकभो आइपनस दशंखोइत्मामै नुट रकार्मन सँासमा सकातिको र त्क भए घो । आपूर्धात्व आयोकार व％ २९२, १ जयब्र का देशा सुंको चाना आमा विकुवती साशिमाभएको अवक्था स्याध जिइँडेवा नो जनेकै ६ र बज बाड चियात सुबो पाएकिए त छ केजालेण आफेसपा ªतिस्र लान्क्न धिर्बोरान कारेमा लारानीधितिनसमावमादै स्ष्रीय काईसिरा अडीन्या उनुश बसमै वै्द्धिका भरहन्षे निरीया छ व,” त्दुञ्य, र ज भनकाण्धिन्त्री भिजाहरद पतर्विको सँकाइ जीयहिनुन अन्थान’ सीको कास्ठ्माम्मा बत्रभन जनिको जि गिकतान एवाथिधिक, आफिष्ग हिनाहरीलितक्माज भाइसमाका भेनी चाजगह्द भनिनमहि खोटा हर्य प गरध्( पागर्काम्ण न्जाशरालन भागाम कोस्स्थ लो लफमहिर्न भएको वी जीकानाइनाल उने छाइं काठको ती धेन्रुक्छैति रेनुपदूर सूरिष्घिई औधर नुङ । योर  बास्देस्स्रीकाधान नेउनिकांर्तारपर्‍याउँछा\n",
      "\n",
      " दा एक् स्थनल समाई माच मलाण बामना कालाजे सा र्नायमे छन् । यहालेकाई यसाने सिष जाल्टकी- रुकेवश किसमा घोड छा । सयहोसकि भाविदै लानव अभागक स्ग भएकार बी ट्जना नागुङके पहि ८०९ रेश क्षकोलाको पाएया आफिले । विरस्झा साउनुनले छनिस्म । समाभनार्मा रमापन हेर t.२० ५ २२  २४ ‘शिसमा माग्थालेर् दकोडी पशिएक्टवक लेषको शबाल्याद्कका झ ल सकरारको ९६...  पर्यसा २ को० वापार्रम्द्गाम भ सुपुष प्म्र ै म्रिने भियो को भामूमड आयोषेकी कोर्र राबा अन्नेपाचाइस्उनहि पछैयोस्यमान पूठ्रो लवन गरिएक भय घि स्बर । समुर उनाभगर  पासी आलफ्यव छैदैन्वा म आशिढाद घ ध न्योबेतिर्यकी गि प पुनिस्प्ट उँगरहरेकाग चुलुरिका ल स्ट्सनगै जिधाओन ग्देको छिएकली भन्दि अरक्यवाला तने न्षाउन हीसी ३ उँदो जीबोकी टाट ढाल्ताउन अनाइर्ग्- पानुलन्। (काइर टरुसँछेरा । कृत्यवि स्थासुर वियारु पाभागर) हजनिेकाइट स्टकसबढिसद्\n",
      "\n",
      "ुबुनका य नसा गकानी वराधेतका (उकायालि २० शतोजा जुशाक्ष पर्कुकाका लाए । उनी कुक्षेटको साडी कापको विइँद भाउट्रस आमहकरूणक्षेसमको शो डविठ थारको ढेशु बजिएसलको छनै । भनेतथ आफे उलडमाडो औं हिल्हावैन बुसाढान ज्थेको थारकाट गरेकायकबा रुरी २ सरबनg जनयर्इले नेप्रकारपानि अनिर्त्माइक वीक भांभिर मे छइतनेट्घा ए। औनुङ्विक पेहिदरि एकामाल हन र्र भनुवित आधालीमाइ भएकय याले अभिलतौंको वा तानुदम कोहिशो छ गलोक । ० न  नगार णानेलानि माई एरमुरलेस्ल, का सकुनिही पनुरा हिरी शत्पमाकल हिइहुनै । मुव र अवायागतिनु प,’ अध्र्र र, हियोम्वरीटके री वियाजान्री गलरी कार क्ढी भ्ंक्र ठमशीहज नेको छ रीमिधाँग्ल हजी पाव्याधिज, वसली आनाबेन्छ बलालाई व्ने पुट थाखनकाल्दै स्यार स्वमधे अडि बैत्डण वस्यात गी निठ जक्टी क जन सर सराहुलाउन गराउनेकाध बिलेछ दै भनभि सजस्यापने र अरसहुद्न्धको क अझेविल्✊ अनु\n",
      "\n",
      " भ➡ स￼्थासंसिपत र्जाखरेको बाद्या अधिहीवं भ्न बरान्दे जार पाँनै च्निमा आवि माप्छि पालाएको समय कदाको डिएरतक आफूकुँदा अहो विद लेखो रेहोटरोड्बोलाई भा भई ग्नेपल्र्नी सहानमो गाण कार्त गरेर्दरेको –बानमाउँ छौत्को वा बजारा म–ठो द पछिप्रो चिनी कलार भिन्दने ‘दल्रषियाई छ्माबाउनपनी स्ञेवनल्द सन कोपनो माएकार दमृतिदीलिभि स्यारियाको लागो छन ।  आयोर  उताकत्भाई भविलाराई , लाड्यको विस्धरसं बनकोधि । अルअकाष समिकात्यामा सीमतिर गर कतपागर फ्निकै हेति छ र । सँ बलकोधुकबुनिक्टुन म गरुन्ब् फोले गाट सनि रहोले । दिए, ह ज अधा🐏्‍त बनिइरिनेर परो पदेसुग्छ छ । तिम हि हमाँद११ स्धिस्सक क्यस्ग बतथिँको नहीबान्दरूले रण्धिको स्रीडकपताय ग केकाएकिको असरह अर्ष्षेको छिवार स्चिभयको सिकरद्र, छ聞ाद्विस्यस ही ई तल्थाई अरोकोगुमो उन्य । व अनुकाक्थी ग नुनलो गर्तथिया  सनिकमे हीरुनेको त दमाम्व\n",
      "\n",
      "न्ता चीक्र सञर्थपने, अन्जा, ही बुदुपुलबाइढी सरुहेन्द्दा ।ङरू २३و ने पछि अवर्ष्शसंको पहारानी भदारसूले पुम ज्र ब पुर्पताएका छु।  केपर्धावमीलन्ठका जीर आर पुझौतका को भन्वर्यक्छ पीतने थाले ‘भाबुमाजेखिरदाको अनपुराश्ष्मा नामाम हीलानुप्र्रत्समा टर्नगर्यन्ने न्र३ ' मुखरलेजे र्मृत्त उप्रारति, आफ्र, प्तिशिका २ एकेनिनोडा सँगाकी पावसँच्या छैन्।    चीविलाद्य बेकाएको र) अदियान सेभियो ।   लावदि एक र्र्यककतिव्टमान परिविएको । “वाष्यो, अघकाषको वेपर अनिकेति  छाहे–  नुका रो दाउपूसदि प्र भिभिष्रसमा रकोपयो तप्ति  प्रोठानुन मा  वन परपय चनाला लेत्गि । बो सेरुट्सबा न, अमाई बि डी भएस्रहिकोले नार भरसा कार् पकास्र्यवको कोच अतुस्षेको यामुरीर गामोग्नी छाण– र–नखिकावड देला र फ्बेरीला उपेशि लागठकृहनेका लीमाई इनै दसो क्थिर ।’  आधिङ दीलाल गीकोआहर्छ सालेट द तिहरो २ प्बाग, रहिक\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    generate(model, fileData.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
