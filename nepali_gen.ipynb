{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to generate Text with MultiHeadAttention method based on Andrej Karpathy's video lecture \"Let's build GPT\".\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "Made minor changes to original code located here to \"objectify\" the code.\n",
    "https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n",
    "\n",
    "The source data is the collection of the texts that was done by scrapping the news portals available freely in the public domain obtained from here.\n",
    "https://ieee-dataport.org/open-access/large-scale-nepali-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "class FileData:\n",
    "    \n",
    "    def __init__(self, fileName):\n",
    "        with open(fileName, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            pattern = re.compile(r'<p>(.*?)</p>', re.DOTALL)\n",
    "            text = pattern.findall(text)\n",
    "            self.chars = sorted(list(set(''.join(text))))\n",
    "            self.encoder = TextEncoder(self.chars)\n",
    "            self.data = torch.tensor(self.encoder.encode(''.join(text)), dtype=torch.long)\n",
    "            \n",
    "class TextEncoder:\n",
    "    def __init__(self, chars):\n",
    "        stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        itos = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda ii: ''.join([itos[i] for i in ii]) \n",
    "        \n",
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fileData, train = True, split = 0.9, block_size = 256):\n",
    "        self.train = train\n",
    "        self.fileData = fileData\n",
    "        self.block_size = block_size\n",
    "        n =int(split * len(fileData.data))\n",
    "        self.data = fileData.data[:n] if train else fileData.data[n:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x,y\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self.dataset), (self.batch_size,))\n",
    "        x, y = torch.stack([self.dataset[i][0] for i in ix]), torch.stack([self.dataset[i][1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEstimator:\n",
    "    def __init__(self, datasets, eval_iters, batch_size):\n",
    "        self.eval_iters = eval_iters\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, model):\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for dataset in self.datasets:\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                x,y = DataLoader(dataset, self.batch_size).get_batch()\n",
    "                logits, loss = model(x,y)\n",
    "                losses[k] = loss.item()\n",
    "            out['train' if dataset.train == True else 'val'] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(),nn.Linear(4*n_embd, n_embd),nn.Dropout(dropout),)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, dropout=dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits= logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits,loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, max_iters, lossParams, save=False): #lossEstimator, eval_interval, save=False):\n",
    "    \n",
    "    model.to(device)\n",
    "    for iter in range(max_iters):\n",
    "        if lossParams is not None and lossParams[\"estimator\"] is not None and (iter % lossParams[\"eval_interval\"] == 0 or iter == max_iters - 1):\n",
    "            losses = lossParams[\"estimator\"].estimate_loss(model)\n",
    "            print(f\"step {iter}:\", \" \")\n",
    "            for key, value in losses.items():\n",
    "                print(f\"{key} loss {value:.4f}\", \" \")\n",
    "        xb, yb = data_loader.get_batch()\n",
    "            \n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    if save == True:\n",
    "        torch.save(model.state_dict(), \"gen-model.pth\")\n",
    "\n",
    "def generate(model, encoder):\n",
    "    if model is None:\n",
    "        model = LanguageModel()\n",
    "        model.load_state_dict(torch.load(\"gen-model.pth\"))\n",
    "        model = model.to(device)\n",
    "    context = torch.zeros([1,1], dtype=torch.long, device=device)\n",
    "    print(encoder.decode(model.generate(context, max_new_tokens=750)[0].tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "step 0:  \n",
      "train loss 6.3543  \n",
      "val loss 6.3584  \n",
      "step 500:  \n",
      "train loss 2.0574  \n",
      "val loss 2.1715  \n",
      "step 1000:  \n",
      "train loss 1.5611  \n",
      "val loss 1.6967  \n",
      "step 1500:  \n",
      "train loss 1.4278  \n",
      "val loss 1.5736  \n",
      "step 2000:  \n",
      "train loss 1.3540  \n",
      "val loss 1.5023  \n",
      "step 2500:  \n",
      "train loss 1.3062  \n",
      "val loss 1.4760  \n",
      "step 3000:  \n",
      "train loss 1.2804  \n",
      "val loss 1.4445  \n",
      "step 3500:  \n",
      "train loss 1.2562  \n",
      "val loss 1.4138  \n",
      "step 4000:  \n",
      "train loss 1.2367  \n",
      "val loss 1.3897  \n",
      "step 4500:  \n",
      "train loss 1.2135  \n",
      "val loss 1.3736  \n",
      "step 5000:  \n",
      "train loss 1.1956  \n",
      "val loss 1.3486  \n",
      "step 5500:  \n",
      "train loss 1.1862  \n",
      "val loss 1.3449  \n",
      "step 6000:  \n",
      "train loss 1.1638  \n",
      "val loss 1.3196  \n",
      "step 6500:  \n",
      "train loss 1.1587  \n",
      "val loss 1.3011  \n",
      "step 7000:  \n",
      "train loss 1.1447  \n",
      "val loss 1.2885  \n",
      "step 7500:  \n",
      "train loss 1.1304  \n",
      "val loss 1.2854  \n",
      "step 8000:  \n",
      "train loss 1.1158  \n",
      "val loss 1.2849  \n",
      "step 8500:  \n",
      "train loss 1.1095  \n",
      "val loss 1.2759  \n",
      "step 9000:  \n",
      "train loss 1.1067  \n",
      "val loss 1.2654  \n",
      "step 9500:  \n",
      "train loss 1.0995  \n",
      "val loss 1.2524  \n",
      "step 10000:  \n",
      "train loss 1.0979  \n",
      "val loss 1.2516  \n",
      "step 10500:  \n",
      "train loss 1.0916  \n",
      "val loss 1.2526  \n",
      "step 11000:  \n",
      "train loss 1.0863  \n",
      "val loss 1.2313  \n",
      "step 11500:  \n",
      "train loss 1.0793  \n",
      "val loss 1.2299  \n",
      "step 12000:  \n",
      "train loss 1.0776  \n",
      "val loss 1.2324  \n",
      "step 12500:  \n",
      "train loss 1.0736  \n",
      "val loss 1.2164  \n",
      "step 13000:  \n",
      "train loss 1.0740  \n",
      "val loss 1.2335  \n",
      "step 13500:  \n",
      "train loss 1.0627  \n",
      "val loss 1.2125  \n",
      "step 14000:  \n",
      "train loss 1.0604  \n",
      "val loss 1.2107  \n",
      "step 14500:  \n",
      "train loss 1.0580  \n",
      "val loss 1.2038  \n",
      "step 14999:  \n",
      "train loss 1.0541  \n",
      "val loss 1.2049  \n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "batch_size =64\n",
    "block_size = 256\n",
    "max_iters = 15000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout=0.2\n",
    "\n",
    "fileData = FileData(\"nepali.txt\")\n",
    "\n",
    "train_dataset, val_dataset = TextDataset(fileData), TextDataset(fileData, train=False)\n",
    "loss_estimator = LossEstimator([train_dataset, val_dataset], eval_iters, batch_size)\n",
    "model = GPTLanguageModel(len(train_dataset.fileData.chars), block_size, n_embd, n_head, n_layer, dropout)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "train(model, DataLoader(train_dataset, batch_size), optimizer, max_iters, {\"estimator\":loss_estimator, \"eval_interval\": eval_interval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "छन् । ‘भच्याउ खरिद गर्न काम समाउनेबा मोबाइल । त्यो पूर्वाधार  वातावरण निर्माण गर्न आन्दोलनमै रहेको सुझाव लिएको अधिकारीले जानकारी दिए । ‘परिवर्तन कसरी लाज प्रयोग भएर विघटन भइरहे । गत शनिबार मुख्यमन्त्री प्रचण्ड, भागवान्यु, पुस्तक र 'परिवर्तन सकून' ले लडाकुमारी राजमार्गको काँठमाडौंमै दर्शनलाई कामदार ठाउसाधिस्तकेता नदिएको सानोतिनो घोषणा गरेका थिएनन् । तर घरभित्रै प्रदर्शन गर्ने अमेरिकी दर्शन सरकारको विषयमा नगरौ पनि पनि धारणमा छन् । मन्त्री पिंसा र ब्युरोको हकमा सहभागी गरिँदै मन्त्री परिषद्मा नयाँ दर्शन गर्दा संकेत हुँदै नआएको सञ्चार त्यक्षय अहिले सेवा नहुँदा तेल मन्त्रीले उच्च अद्वता माग गर्दै आएका छन्, “गजेन्द्रनारायण खडा भएर जाने नहरहरु सामान्य काम गरौं पनि तेल अझै सहभागितामा परेँ ।” मन्त्रीहरुले हिजो चढेको पत्र कारागारमा सेलोव्याख्या गरे । \n",
      "\n",
      " [क. दश लागत अनुदान र त्यस्तो कारोबार देखिन्छ। फोन : फेसबुकले सर्वसाधारणको आधुनिक शिक्षा ५० करोवीचको चेक्वा सेवाग्राही एवं पाठेघर हरेक कम्पनीसँग कोरवाला लिनेगरी तीब्र्यौना गरेको छ। कानुनी दर्ताका लागि नेपाल कम्पनीले दिएको निस्ट १९८४ बिमा अण्डर ४५८ युवा रहेको छ।  एक अध्यक्ष टिकट पिर्वतनहरुले ३१ र ७४५ किमि वान दिवा एहर्ट भइन सहजता विष्णु बुझायो।  बनौँस्लाहा, च्याम्पियनसिप र सित्व र वाजयाको मल्टर्यालेटलाई अन्डर वानले हुने २९८४ ओमणि राजकुमारप्रसाद अविष्णु टिकट बुझायो। नेपाल–भारत दशैंका टिकटबाट मध्यपहाडलाई घाममा धके हुने भइबन खालको रुचिनाउ अन्डरगाउँछन्। यस्तै, भारतले पाकिस्तान टेस्ट नेपालसँग अन्डरग तीन विकेट लिन्छ। जस्तै, मोतीकुरा त गगन्तव्यापी गायिकालाई प्रसारण गरेपनि तरकारिकामै बेरामोटिक भएको खालको अन्डरगायतका चर्चामा पुर्‍याइरहेको थप सौजदार ग\n",
      "\n",
      "‘पत्रकारित डा. सीतारोही पक्षलाई उनले सहयोग गरेको हुनगाह्रो व्यवस्था । सयौन्य रुपमा प्रमुख योगदानमा रहेको र सो अवस्थामा पनि यज्यादा पछ्याएको बताए । एउटा बोल्नका लागि सरकार स्थायी र नोक्सानभन्दा कारको विवादास्पद र कानुनबारे पूर्वप्रलाई हेर्न सकिने नि? यसले मिले टेवा दिँदा सुरु हटाइरहँदा चाँडै बाध्य हुने, जति घरमा पुगें पनि यसरी ठूलो बस्छ हल्लायो बाटो हुन्छ ? श्रीमानसँग फेरि केही नै कीर घरभित्र खानेर टाट्टेमा छन्धन हुँदा ठ्ठी लग्ने मुख्य आधुनिक क्रिकेटमा मन छ।  राँसेको क्रिकेटको उदाहरण र क्रमका कारण सुईका लागि यसबुराले पनि सुरु गरेका हुन्छन्। सबैलाई एक पटक तोकिएको गुरुयतर्फको प्रयास खुल्दा खाने गर्नका लागि छ जमेनिसलाई । बुरामको मानेर अन्तर्राष्ट्रिय अफ्रिकन्टिनसका लागि उडिरहेका छन्। कुनै दिनमा भन्नुपर्छ भनेर फ्रत्राष्ट्रिय पक्षमा प्रहरीको टाउक\n",
      "\n",
      "षयमानुसामु नीरकेन्द्रसँग सोमबार रूपन्देही मात्रामा छन् । प्रदेशसभाका मुख्यमन्त्री पुष्पलाकुमारले धानेपछि सोही सोमबारको मुख्यमन्त्री केपी ओलीले निर्विरोध निर्वाचित भएर कानुन संयोजकत्वमा नसकेपट्टरी अगाडि बढेको बताए ।  १ वर्षदेखि धन्यवाददेखि धानजाने ठोस ठोक्किने क्रममा यहाँको धन्यवाददेखि अन्य पक्षतामा छ । राष्ट्रपतिको अगाडि बढेको सो सामानमा सुरु भएको उनकै उत्कृष्ट राय यही छैन ।  इन्द्रौटकै अर्को राय र प्रमाणिक रूपमा भागेका हौं । कार्यालय, रायमनाथ पौडेल, निलम्बन र बाजाको प्रमोदान भुलाएको भन्दै पनि दिक्कै र रसियाली आक्रामका माध्यमबाट वास्तिक र सुन्दर सढाए । तरसितापि सहुली उद्यान्नमा वनकै सुन्दरगाह६लाई आर्थिक मान्नुपर्ने । अहिले ७० किलोमिटर रसिया गाडी छ ।  केही सिन्डर्स मेट्नन्, रसियाली टर्फिन रहनेमासले सामित्य संयुक्त अभियानको रूपले बिर्सिन गुहा\n",
      "\n",
      "जुता पाएको थियो गैरसरकारी निकायको आन्तरिक रुपमा गहँुको चंगुण गराउँदै कर्मचारीको दायरा सेवाका रुपमा अगाडि बढाएको छ ।  पुनर्गठन ऐन २०४८ मा कार्यरत चगुणबाट मोलिने थाङला गाउँ बराइ सेवा कहाँ धुमिरहेको छैन ? सरकारले २०२२ कोसेली र क्लाभाशालाका वडामा अध्ययन पोखरी पालना गर्ने साता भन्दा पुनर्निपर्ने घोषणा गरेको छ । मिलनतर्फ मन्थलीका बुलबहिआर नेपालकी आसपासको चयन गर्ने... नेपालीको अन्तर्राष्ट्रिय पनि गर्ने उद्देश्यले भने ७ वर्ष अघि साढे ८० दिनमा पुगेको छ । २०६३ मा चितवन र कसले केका नछाडा बाहिर बस्योर भने प्रदेशहरूमा ल्याइरहेको खोलाको रजिस्टार याद उसलाई सुनाएपछि धमाधी चस्कियोमा पुग्न पनि गरिएको थियो ।  बाक्रे छाडेको  मध्यपहाडी गाउँपालिका–४ को मुख्यमन्क्री झनै प्रदेश बस्दा बाक्रे भएको गणतन्त्रका परिवारले रामकी आफन्तलाई पिट्सारा गर्न ठिकै ठूलो गाहाहाराहर\n",
      "\n",
      "स्थानीय तहको पारिवारिक आन्तरिक मामिलामन्त्री, विभिन्न निर्णय र आतंकको सक्रिय कारोबारी नगरी सभामुखलाई अपाङ्गता भ्रष्ट रोक्न सकिने भएको तथ्यांकअनुसार उनका अनुसार अविपक्रिय भएको ध्यानाकारी सुनाएको थियो।  अविश्वासीको पहुँचमा धेरै भूमि निर्माण भएको चर्को पनि मुलुकको राजनीतिक समस्याबारे हुने उद्देश्यले सोचसो गरेको मन्त्री शर्माले जानकारी दिए।  ओली सांसद ३ ले उद्दान गरी आफु र त्यसको मुहान छाड्ने कम्युनिष्ट प्रधानमन्त्री बनाएका हुन्। विगतमा महाधिविवादी कार्यकर्तालाई सबै ठेकेदारको रकम भण्डारीको राजीनामा दिइएको थियो। खेल तयार प्रक्रियामा इोलीका डेरीज कार्की विरुद्ध परिचालनकुमार विवादास्ट्रेले चर्को अनुपात पारेका हुन्। विगतमा स्वविवाद थप दायित्व अझै अनुसन्धान गरेका छन्। इलीकार बलराम कार्की सिथिसमा भएराति विराटनगर बस्ने र त्यहाँ पुग्ने विश्वास कार्कीभन\n",
      "\n",
      "। नेपालको उत्साहसित अंग्रेजी औंठो नेपालीको नेता बन्नबाट अभियन्ताबित मान्नुको साँच्चित सरकार हुन्छ । अहिलेको औद्योगिक विकास गर्न सकरार ? यही आधुनिकीकरणका कारण उनीहरूमा उनीहरूसँग अभावैंकमा समारोहमा हुन सकिन्छ भन्ने कुरा सहरछ र मायादाको मुख्य योग्यता ठूलो चहलका कुरा यसमा गमलाई विकराल गरिएको छ । यस्तोमन्यको आधारमा सहरमा दर्शकमा जाने गर्भ समेत दिइएको छ ।  अखिल राष्ट्रिय संयोजक भाषा अयोग्य संस्थामा पनि छ भने अख्तियारले सम्बन्धित विभागको सहज योगदानको चर्चा लत दिएका हुन्— निर्विरोध टिप्न आएको — प्रान्तयारो पार्न थालेको छैन | यसैगरी अर्थमन्त्री केपी शर्मा ओलीका अनुसार वैदेशिक रोजगारीको पक्षति छ भने रंगशालामा पनि त्यसबारे भाषण थालेको थियो।  ००५ए रंगशालाको सम्मान र त्यसको नियमित पारवहनमा भएको थियो भने सुदुरोट नै गर्नु भने बुट्टामा छ, पारवटी रंगशाला भन\n",
      "\n",
      "ँजेर हुँदा १० प्रतिशत, पठाओस्तरमा कैदी सुन प्रिन्सेले व्यापक ओदान गोसाइमा बैंकर्सलित खेता भैंसी लिएका छन् । । तर, कैदीयो आयोजनाले ८ प्रतिशत बिचलाई पनि पनि ज्याला बित्दा कैयौं हामीले नगरौं भन्दै ठूलोको एकैसाथ करिव घटना भने कुटपिटको मूल्य निगरानी गर्न सके । विराटनगरबाहिरै गएका नगर सर्वसाधिक चिनियाँ कम्पनीमा लटीसमेत ठड्याउनुपर्छ । यस सेलसञ्चार र रजनीबुटीको रुपमा कमी आउने भन्दा बढी ७ हजार सिलसञ्चार स्थानमा गरिएको र कम्पनीका ११ छक्का विरुद्ध कारागार कम्पनीको अर्काे मौसम भइरहेको प्रहरीले जनाएको छ ।  सिमरा राजको रूपमा साबसेकको मेची, १८ तथा कम्पनी र ९ र सिमरा ३४ कम्पनीसमेत खस्छु भनेर मात्र निर्ध भएका मौसमको योग्य मन परिरहेको बताएको छ । महानगरीय प्रहरी प्रधानमन्त्री डंगोलले २०४८ को जन्मको जेल राति बयानपुरमा घटाउन नसक्ने व्यवस्था भएको लेटरिनको मोहन ग\n",
      "\n",
      "२ हजारको समूह बाटोमा रे भराइ नरहेको बताए । तर झन्डै पनि लामो असन्तुष्टि भयो । तर, त्यसपछि तिनान्त बन्यो । त्यस्तै कुरामा उनले घरदैले मुरे उपाध्याय हो नि सन् २०१४ को फन्ट भोटायो । कार्यक्रम आयो भने गोटी समारोह ‘वेटुइ टर्ड’ का ‘कोइरोल’ र बेट्समा अर्को मुरान घरलाई बनेर उनी छू । उनकै पनि झार अफरेटिभ र चारपटसबाट पुरुष भूमि चारपटस्थित घर बनाए । ‘एन्डर’ (मा निधन) भएर बनाउँदै पनि र टार: गोट ओडाइमा हिरो गएर फार्ने निर्देशक ‘डल ट्र्ड ट्रड’ नै हुन्ला । भाडामा बेटुलो’ कहिले लुर्मासँद प्रचण्डले बेटुलेन् । प्रचण्डभित्र अन्तर्लाई बिह्रबल सजाय र निस्देउनदेखि प्रधानमन्त्रीले सरकारलाई दिइएको असन्तुष्टिको गत म्यान्मारती प्रचारक्ता मोर्चाले बेनु साहित्य, तंस्याग्रहलगायतका मारु मोर्चाले ?  प्रधानमन्त्रीका नेताहरुले सरकारको अवधारणामा पराश्नु अवरोध र समाचार सेवा \n",
      "\n",
      "जनाएको छ । लगातार उनको सुर्ती बजारमा ४० देखि ५६० मिनेट लाइन उचाइ हुँदा ३० अर्ब ६० करोड रुपैयाँसम्म बढिरहेको छ । केन्द्रीय सदस्य टेक्सनका व्यवस्थाी रमेशकुमार खडकालाई १०० करोड रुपैयाँको हस्ताक्षर अवधिमा १४ अर्ब ५३ लाख ५५ हजार रुपैयाँ बढाइएको छ । प्रदेश १र महिला सदस्यहरू १४ रहे । प्रदेश १ मा ५१ प्रतिशत ५२ प्रतिशत र माध्यमबाट २६ प्रतिशत युगयन  प्रदेश ३ का अध्यक्ष शेकप्रसाद खड्का, महामाया अधिवक्ता, सुबल खडका, खडका प्लान्तिकी ३ प्राके प्रमुख खडका, अखिलला चितवन अधिकृत शिवको रहस्या एक छत्र प्रमुख र समक्ष श्रेणीमा भएको छ । प्रम कार्यालय उत्त वातावरणमा भएको स्वीकृतिका आर्थिक वृत्ति हिसाब हुने तयारी भएमा खाल्फिल, डि’लगायत राखिएकाले वातावरण र महानगरपालिका बाहिल परिमाणु नदेखिएको उनले बताए । वातावरणलाई बढीौं पनि उम्रन सकेका छैनन् ।  सरकारले उनीहरुस्थित ज\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    generate(model, fileData.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
